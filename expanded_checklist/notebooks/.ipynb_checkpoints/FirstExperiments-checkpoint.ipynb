{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import checklist\n",
    "from checklist.test_suite import TestSuite\n",
    "import checklist.editor\n",
    "import checklist.text_generation\n",
    "from checklist.test_types import MFT, INV, DIR\n",
    "from checklist.expect import Expect\n",
    "import numpy as np\n",
    "import spacy\n",
    "from checklist.test_suite import TestSuite\n",
    "from checklist.perturb import Perturb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<checklist.text_generation.TextGenerator at 0x7fb4744695b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor = checklist.editor.Editor()\n",
    "editor.tg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp_models.classification.dataset_readers.stanford_sentiment_tree_bank import StanfordSentimentTreeBankDatasetReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_reader = StanfordSentimentTreeBankDatasetReader(use_subtrees=False, granularity='2-class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '1', '1', '1', '1']\n",
      "[\"It 's a lovely film with lovely performances by Buy and Accorsi .\", \"And if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\", 'A warm , funny , engaging film .', 'Uses sharp humor and insight into human nature to examine class conflict , adolescent yearning , the roots of friendship and sexual identity .', 'Entertains by providing good , lively company .']\n"
     ]
    }
   ],
   "source": [
    "ROOT = \"/home/ubuntu/workplace/ComprehendBiasTools\"\n",
    "dev_sst_path = f\"{ROOT}/datasets/SST/trees/dev.txt\"\n",
    "test_sst_path = f\"{ROOT}/datasets/SST/trees/test.txt\"\n",
    "\n",
    "\n",
    "labels = []\n",
    "text_data = []\n",
    "\n",
    "gen = sst_reader._read(dev_sst_path)\n",
    "for i, instance in enumerate(gen):\n",
    "    tokens = instance['tokens'].tokens\n",
    "    label = instance['label'].label\n",
    "\n",
    "    text = ' '.join([x.text for x in tokens])\n",
    "\n",
    "    text_data.append(text)\n",
    "    labels.append(label)\n",
    "\n",
    "print(labels[:5])\n",
    "print(text_data[:5])\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "parsed_data = list(nlp.pipe(text_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = TestSuite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MFT VOCABULARY FROM CHECKLIST SENTIMENT SUITE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a test suite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "air_noun already in lexicons. Call with overwrite=True to overwrite",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b830d59c150c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mair_noun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'flight'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'seat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pilot'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'staff'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'service'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'customer service'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'aircraft'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'plane'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'food'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cabin crew'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'company'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'airline'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'crew'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0meditor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_lexicon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'air_noun'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mair_noun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpos_adj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'good'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'great'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'excellent'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'amazing'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'extraordinary'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'beautiful'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fantastic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'incredible'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'exceptional'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'awesome'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'perfect'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fun'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'happy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'adorable'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'brilliant'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'exciting'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sweet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wonderful'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mneg_adj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'awful'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bad'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'horrible'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weird'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rough'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lousy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'unhappy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'average'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'difficult'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'poor'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sad'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'frustrating'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hard'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lame'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nasty'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'annoying'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'boring'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'creepy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dreadful'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ridiculous'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'terrible'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ugly'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'unpleasant'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/biastools/lib/python3.8/site-packages/checklist/editor.py\u001b[0m in \u001b[0;36madd_lexicon\u001b[0;34m(self, name, values, overwrite)\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;31m# words can be strings, dictionarys, and other objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexicons\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s already in lexicons. Call with overwrite=True to overwrite'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexicons\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: air_noun already in lexicons. Call with overwrite=True to overwrite"
     ]
    }
   ],
   "source": [
    "air_noun = ['flight', 'seat', 'pilot', 'staff', 'service', 'customer service', 'aircraft', 'plane', 'food', 'cabin crew', 'company', 'airline', 'crew']\n",
    "editor.add_lexicon('air_noun', air_noun)\n",
    "\n",
    "pos_adj = ['good', 'great', 'excellent', 'amazing', 'extraordinary', 'beautiful', 'fantastic', 'nice', 'incredible', 'exceptional', 'awesome', 'perfect', 'fun', 'happy', 'adorable', 'brilliant', 'exciting', 'sweet', 'wonderful']\n",
    "neg_adj = ['awful', 'bad', 'horrible', 'weird', 'rough', 'lousy', 'unhappy', 'average', 'difficult', 'poor', 'sad', 'frustrating', 'hard', 'lame', 'nasty', 'annoying', 'boring', 'creepy', 'dreadful', 'ridiculous', 'terrible', 'ugly', 'unpleasant']\n",
    "neutral_adj = ['American', 'international',  'commercial', 'British', 'private', 'Italian', 'Indian', 'Australian', 'Israeli', ]\n",
    "editor.add_lexicon('pos_adj', pos_adj, overwrite=True)\n",
    "editor.add_lexicon('neg_adj', neg_adj, overwrite=True )\n",
    "editor.add_lexicon('neutral_adj', neutral_adj, overwrite=True)\n",
    "\n",
    "pos_verb_present = ['like', 'enjoy', 'appreciate', 'love',  'recommend', 'admire', 'value', 'welcome']\n",
    "neg_verb_present = ['hate', 'dislike', 'regret',  'abhor', 'dread', 'despise' ]\n",
    "neutral_verb_present = ['see', 'find']\n",
    "pos_verb_past = ['liked', 'enjoyed', 'appreciated', 'loved', 'admired', 'valued', 'welcomed']\n",
    "neg_verb_past = ['hated', 'disliked', 'regretted',  'abhorred', 'dreaded', 'despised']\n",
    "neutral_verb_past = ['saw', 'found']\n",
    "editor.add_lexicon('pos_verb_present', pos_verb_present, overwrite=True)\n",
    "editor.add_lexicon('neg_verb_present', neg_verb_present, overwrite=True)\n",
    "editor.add_lexicon('neutral_verb_present', neutral_verb_present, overwrite=True)\n",
    "editor.add_lexicon('pos_verb_past', pos_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neg_verb_past', neg_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neutral_verb_past', neutral_verb_past, overwrite=True)\n",
    "editor.add_lexicon('pos_verb', pos_verb_present+ pos_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neg_verb', neg_verb_present + neg_verb_past, overwrite=True)\n",
    "editor.add_lexicon('neutral_verb', neutral_verb_present + neutral_verb_past, overwrite=True)\n",
    "\n",
    "test = MFT(pos_adj + pos_verb_present + pos_verb_past, labels=2)\n",
    "suite.add(test, 'single positive words', 'Vocabulary', '')\n",
    "\n",
    "test = MFT(neg_adj + neg_verb_present + neg_verb_past, labels=0)\n",
    "suite.add(test, 'single negative words', 'Vocabulary', '')\n",
    "\n",
    "test = MFT(neutral_adj + neutral_verb_present + neutral_verb_past, labels=1)\n",
    "suite.add(test, 'single neutral words', 'Vocabulary', 'TODO_DESCRIPTION')\n",
    "\n",
    "t = editor.template('{it} {air_noun} {be} {pos_adj}.', it=['The', 'This', 'That'], be=['is', 'was'], labels=2, save=True)\n",
    "t += editor.template('{it} {be} {a:pos_adj} {air_noun}.', it=['It', 'This', 'That'], be=['is', 'was'], labels=2, save=True)\n",
    "t += editor.template('{i} {pos_verb} {the} {air_noun}.', i=['I', 'We'], the=['this', 'that', 'the'], labels=2, save=True)\n",
    "t += editor.template('{it} {air_noun} {be} {neg_adj}.', it=['That', 'This', 'The'], be=['is', 'was'], labels=0, save=True)\n",
    "t += editor.template('{it} {be} {a:neg_adj} {air_noun}.', it=['It', 'This', 'That'], be=['is', 'was'], labels=0, save=True)\n",
    "t += editor.template('{i} {neg_verb} {the} {air_noun}.', i=['I', 'We'], the=['this', 'that', 'the'], labels=0, save=True)\n",
    "# equivalent to:\n",
    "# test = MFT(t.data, labels=t.labels, templates=t.templates)\n",
    "test = MFT(**t)\n",
    "suite.add(test, 'Sentiment-laden words in context', 'Vocabulary', 'Use positive and negative verbs and adjectives with airline nouns such as seats, pilot, flight, etc. E.g. \"This was a bad flight\"')\n",
    "\n",
    "t = editor.template('{it} {air_noun} {be} {neutral_adj}.', it=['That', 'This', 'The'], be=['is', 'was'], save=True)\n",
    "t += editor.template('{it} {be} {a:neutral_adj} {air_noun}.', it=['It', 'This', 'That'], be=['is', 'was'], save=True)\n",
    "t += editor.template('{i} {neutral_verb} {the} {air_noun}.', i=['I', 'We'], the=['this', 'that', 'the'], save=True)\n",
    "test = MFT(t.data, labels=1, templates=t.templates)\n",
    "suite.add(test, 'neutral words in context', 'Vocabulary', 'Use neutral verbs and adjectives with airline nouns such as seats, pilot, flight, etc. E.g. \"The pilot is American\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for test in suite.tests:\n",
    "    suite.tests[test].name = test\n",
    "    suite.tests[test].description = suite.info[test]['description]']\n",
    "    suite.tests[test].capability = suite.info[test]['capability']\n",
    "    \n",
    "path = f'{ROOT}/pjc_sentiment_suite.pkl'\n",
    "suite.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING AND RUNNING TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./get_predictions.sh exp=experiments/roberta-sst-2-test.jsonnet data=/tmp/temp2 OUTDIR=predictions/sst2_with_neut\n"
     ]
    }
   ],
   "source": [
    "path = f'{ROOT}/pjc_sentiment_suite.pkl'\n",
    "suite = TestSuite.from_file(path)\n",
    "\n",
    "suite_sents = \"/tmp/sents_to_process.txt\"\n",
    "suite.to_raw_file(suite_sents, n=500, seed=1)\n",
    "\n",
    "model_name = \"roberta-sst-2-pred-all\"\n",
    "data_name = suite_sents.split(\"/\")[-1]\n",
    "out_dir = \"predictions/sst2_with_neut\"\n",
    "\n",
    "bashCommand = f\"./get_predictions.sh exp=experiments/{model_name}.jsonnet data={suite_sents} OUTDIR={out_dir}\"\n",
    "print(bashCommand)\n",
    "\n",
    "import subprocess\n",
    "process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE, cwd=ROOT)\n",
    "output, error = process.communicate()\n",
    "pred_path = f'{ROOT}/{out_dir}/{model_name}-{data_name}.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/workplace/ComprehendBiasTools/predictions/sst2_with_neut/roberta-sst-2-test-temp2.txt\n"
     ]
    }
   ],
   "source": [
    "print(pred_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait as we prepare the table data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae6c59fe95c45f496ab4773385e78e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SuiteSummarizer(stats={'npassed': 0, 'nfailed': 0, 'nfiltered': 0}, test_infos=[{'name': 'single positive wordâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "suite.run_from_file(pred_path, overwrite=True)\n",
    "suite.visual_summary_table()\n",
    "\n",
    "# suite.summary() # or suite.visual_summary_table()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_biastools)",
   "language": "python",
   "name": "conda_biastools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
